#!/usr/bin/env python3
"""
Issuu PDFä¸‹è½½å™¨ä¸»å…¥å£è„šæœ¬

ä½¿ç”¨æ–¹æ³•:
    python download.py

ç¯å¢ƒå˜é‡é…ç½®:
    HTTP_PROXY              - ä»£ç†URL (å¦‚: http://proxy.example.com:8080)
    PAGE_LOAD_TIMEOUT       - é¡µé¢åŠ è½½è¶…æ—¶æ—¶é—´(æ¯«ç§’, é»˜è®¤: 60000)
    DISABLE_IMAGES          - æ˜¯å¦ç¦ç”¨å›¾ç‰‡ (é»˜è®¤: true)
    HEADLESS                - æ˜¯å¦æ— å¤´æ¨¡å¼ (é»˜è®¤: true)
    MAX_RETRIES             - æœ€å¤§é‡è¯•æ¬¡æ•° (é»˜è®¤: 3)
    DEBUG                   - è°ƒè¯•æ¨¡å¼ (é»˜è®¤: false)
"""

import os
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from scrapers import IssuuCrawler


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®å‚æ•°
    url = "https://issuu.com/vidula.dinesh/docs/vidula_dinesh_issuu"
    save_dir = os.path.join(os.path.dirname(__file__), 'downloads')
    save_path = os.path.join(save_dir, 'vidula_dinesh_issuu.pdf')

    # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼ï¼Œä¹Ÿå¯é€šè¿‡å‚æ•°è¦†ç›–ï¼‰
    crawler = IssuuCrawler(
        max_retries=None,      # ä½¿ç”¨é…ç½®æ–‡ä»¶å€¼
        timeout=None,          # ä½¿ç”¨é…ç½®æ–‡ä»¶å€¼
        min_delay=None,        # ä½¿ç”¨é…ç½®æ–‡ä»¶å€¼
        max_delay=None,        # ä½¿ç”¨é…ç½®æ–‡ä»¶å€¼
        headless=None,         # ä½¿ç”¨é…ç½®æ–‡ä»¶å€¼
        proxy_url=None,        # ä½¿ç”¨é…ç½®æ–‡ä»¶å€¼
        disable_images=None,   # ä½¿ç”¨é…ç½®æ–‡ä»¶å€¼
    )

    # æ‰§è¡Œä¸‹è½½
    success = crawler.download_pdf(url, save_path)

    if success:
        print(f"\nâœ… PDFä¸‹è½½æˆåŠŸ!")
        print(f"ğŸ“ æ–‡ä»¶ä¿å­˜ä½ç½®: {save_path}")
        sys.exit(0)
    else:
        print(f"\nâŒ PDFä¸‹è½½å¤±è´¥")
        print(f"\nğŸ’¡ æç¤º:")
        print(f"   - å¦‚é‡ç½‘ç»œé—®é¢˜ï¼Œå¯å°è¯•è®¾ç½®ä»£ç†: export HTTP_PROXY=http://your-proxy:port")
        print(f"   - å¯ç”¨è°ƒè¯•æ¨¡å¼æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯: export DEBUG=true")
        print(f"   - è°ƒæ•´è¶…æ—¶æ—¶é—´: export PAGE_LOAD_TIMEOUT=90000")
        sys.exit(1)


if __name__ == "__main__":
    main()
