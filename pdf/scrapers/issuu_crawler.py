"""
Issuu PDFçˆ¬è™«æ¨¡å—

ä½¿ç”¨Playwrightè‡ªåŠ¨åŒ–ä¸‹è½½Issuuä¸Šçš„PDFæ–‡æ¡£
æ”¯æŒUser-Agentè½®æ¢ã€éšæœºå»¶è¿Ÿã€å¤šçº§ç­‰å¾…ç­–ç•¥å’Œé”™è¯¯é‡è¯•æœºåˆ¶
"""

import os
import random
import re
import time
import logging
from typing import Optional
from pathlib import Path

import requests
from playwright.sync_api import sync_playwright, Page, TimeoutError as PlaywrightTimeoutError

# å¯¼å…¥é…ç½®
try:
    from config import (
        WAIT_STRATEGIES,
        PROXY_URL,
        PAGE_LOAD_TIMEOUT,
        DISABLE_IMAGES,
        BROWSER_ARGS,
        HEADLESS,
        MAX_RETRIES,
        MIN_DELAY,
        MAX_DELAY,
        VIEWPORT_WIDTH,
        VIEWPORT_HEIGHT,
        TIMEZONE_ID,
        LOCALE,
        DEBUG,
    )
except ImportError:
    # å¦‚æœé…ç½®æ¨¡å—ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å€¼
    WAIT_STRATEGIES = ['domcontentloaded', 'load', 'networkidle']
    PROXY_URL = None
    PAGE_LOAD_TIMEOUT = 60000
    DISABLE_IMAGES = True
    BROWSER_ARGS = [
        '--disable-blink-features=AutomationControlled',
        '--no-sandbox',
        '--disable-setuid-sandbox',
    ]
    HEADLESS = True
    MAX_RETRIES = 3
    MIN_DELAY = 2.0
    MAX_DELAY = 5.0
    VIEWPORT_WIDTH = 1920
    VIEWPORT_HEIGHT = 1080
    TIMEZONE_ID = 'America/New_York'
    LOCALE = 'en-US'
    DEBUG = False

# é…ç½®æ—¥å¿—
log_level = logging.DEBUG if DEBUG else logging.INFO
logging.basicConfig(
    level=log_level,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class IssuuCrawler:
    """Issuu PDFä¸‹è½½çˆ¬è™«ç±»"""

    # User-Agentåˆ—è¡¨ï¼Œç”¨äºè½®æ¢
    USER_AGENTS = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    ]

    def __init__(
        self,
        max_retries: Optional[int] = None,
        timeout: Optional[int] = None,
        min_delay: Optional[float] = None,
        max_delay: Optional[float] = None,
        headless: Optional[bool] = None,
        proxy_url: Optional[str] = None,
        disable_images: Optional[bool] = None,
    ):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶å€¼ï¼‰
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶å€¼ï¼‰
            min_delay: æœ€å°éšæœºå»¶è¿Ÿï¼ˆç§’ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶å€¼ï¼‰
            max_delay: æœ€å¤§éšæœºå»¶è¿Ÿï¼ˆç§’ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶å€¼ï¼‰
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶å€¼ï¼‰
            proxy_url: ä»£ç†URLï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶å€¼ï¼‰
            disable_images: æ˜¯å¦ç¦ç”¨å›¾ç‰‡ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶å€¼ï¼‰
        """
        self.max_retries = max_retries if max_retries is not None else MAX_RETRIES
        self.timeout = timeout if timeout is not None else PAGE_LOAD_TIMEOUT
        self.min_delay = min_delay if min_delay is not None else MIN_DELAY
        self.max_delay = max_delay if max_delay is not None else MAX_DELAY
        self.headless = headless if headless is not None else HEADLESS
        self.proxy_url = proxy_url if proxy_url is not None else PROXY_URL
        self.disable_images = disable_images if disable_images is not None else DISABLE_IMAGES

        logger.info("=" * 60)
        logger.info("Issuuçˆ¬è™«é…ç½®:")
        logger.info(f"  æœ€å¤§é‡è¯•æ¬¡æ•°: {self.max_retries}")
        logger.info(f"  è¶…æ—¶æ—¶é—´: {self.timeout}ms ({self.timeout/1000:.1f}s)")
        logger.info(f"  å»¶è¿ŸèŒƒå›´: {self.min_delay}-{self.max_delay}ç§’")
        logger.info(f"  æ— å¤´æ¨¡å¼: {self.headless}")
        logger.info(f"  ä»£ç†é…ç½®: {self.proxy_url or 'æœªè®¾ç½®'}")
        logger.info(f"  ç¦ç”¨å›¾ç‰‡: {self.disable_images}")
        logger.info(f"  ç­‰å¾…ç­–ç•¥: {WAIT_STRATEGIES}")
        logger.info("=" * 60)

    def _get_random_user_agent(self) -> str:
        """è·å–éšæœºUser-Agent"""
        return random.choice(self.USER_AGENTS)

    def _random_delay(self) -> None:
        """éšæœºå»¶è¿Ÿ"""
        delay = random.uniform(self.min_delay, self.max_delay)
        logger.debug(f"å»¶è¿Ÿ {delay:.2f} ç§’")
        time.sleep(delay)

    def _setup_page(self, page: Page) -> None:
        """
        é…ç½®é¡µé¢è®¾ç½®

        Args:
            page: Playwrighté¡µé¢å¯¹è±¡
        """
        # è®¾ç½®è§†å£å’Œæ—¶åŒº
        page.set_viewport_size({'width': VIEWPORT_WIDTH, 'height': VIEWPORT_HEIGHT})
        
        # ç¦ç”¨å›¾ç‰‡åŠ è½½
        if self.disable_images:
            def handle_route(route, request):
                if request.resource_type in ['image', 'media', 'font']:
                    route.abort()
                else:
                    route.continue_()
            page.route('**/*', handle_route)
            logger.debug("å·²ç¦ç”¨å›¾ç‰‡ã€åª’ä½“å’Œå­—ä½“åŠ è½½")

    def _open_page_with_strategy(self, page: Page, url: str) -> bool:
        """
        ä½¿ç”¨å¤šçº§ç­‰å¾…ç­–ç•¥æ‰“å¼€é¡µé¢

        Args:
            page: Playwrighté¡µé¢å¯¹è±¡
            url: è¦æ‰“å¼€çš„URL

        Returns:
            æ˜¯å¦æˆåŠŸæ‰“å¼€é¡µé¢
        """
        page.set_default_timeout(self.timeout)

        for strategy_index, strategy in enumerate(WAIT_STRATEGIES):
            strategy_name = strategy
            attempt_start = time.time()
            
            try:
                logger.info(f"ä½¿ç”¨ç­‰å¾…ç­–ç•¥: {strategy_name} (é˜¶æ®µ {strategy_index + 1}/{len(WAIT_STRATEGIES)})")
                page.goto(url, wait_until=strategy, timeout=self.timeout)
                
                elapsed = time.time() - attempt_start
                logger.info(f"âœ… é¡µé¢åŠ è½½æˆåŠŸ ({strategy_name}) - è€—æ—¶: {elapsed:.2f}ç§’")
                return True
                
            except PlaywrightTimeoutError as e:
                elapsed = time.time() - attempt_start
                logger.warning(f"â±ï¸  ç­‰å¾…ç­–ç•¥ '{strategy_name}' è¶…æ—¶ - è€—æ—¶: {elapsed:.2f}ç§’")
                
                # å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªç­–ç•¥ï¼Œå°è¯•ä»å·²åŠ è½½çš„DOMä¸­æå–PDF
                if strategy_index < len(WAIT_STRATEGIES) - 1:
                    logger.info("å°è¯•ä»å·²åŠ è½½çš„DOMä¸­æå–PDFé“¾æ¥...")
                    pdf_url = self._extract_pdf_url_from_dom(page)
                    if pdf_url:
                        logger.info(f"âœ… ä»DOMä¸­æˆåŠŸæå–PDFé“¾æ¥")
                        # å°†PDF URLå­˜å‚¨åœ¨pageå¯¹è±¡ä¸­
                        page.pdf_url = pdf_url
                        return True
                    logger.info("DOMä¸­æœªæ‰¾åˆ°PDFé“¾æ¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªç­‰å¾…ç­–ç•¥...")
                else:
                    # æœ€åä¸€ä¸ªç­–ç•¥ä¹Ÿå¤±è´¥äº†ï¼Œæœ€åä¸€æ¬¡å°è¯•æå–
                    logger.info("æœ€åå°è¯•ä»å·²åŠ è½½çš„DOMä¸­æå–PDFé“¾æ¥...")
                    pdf_url = self._extract_pdf_url_from_dom(page)
                    if pdf_url:
                        logger.info(f"âœ… ä»DOMä¸­æˆåŠŸæå–PDFé“¾æ¥")
                        page.pdf_url = pdf_url
                        return True
                    
            except Exception as e:
                elapsed = time.time() - attempt_start
                logger.error(f"âŒ é¡µé¢åŠ è½½å¤±è´¥ ({strategy_name}) - è€—æ—¶: {elapsed:.2f}ç§’ - é”™è¯¯: {e}")
                if strategy_index < len(WAIT_STRATEGIES) - 1:
                    continue
                return False

        return False

    def _extract_pdf_url_from_dom(self, page: Page) -> Optional[str]:
        """
        ä»DOMä¸­æå–PDFé“¾æ¥ï¼ˆç”¨äºéƒ¨åˆ†åŠ è½½æƒ…å†µï¼‰

        Args:
            page: Playwrighté¡µé¢å¯¹è±¡

        Returns:
            PDFä¸‹è½½é“¾æ¥ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å›None
        """
        logger.debug("å¼€å§‹ä»DOMä¸­æå–PDFé“¾æ¥...")
        
        # è·å–é¡µé¢å†…å®¹
        try:
            page_content = page.content()
            
            # æ–¹æ³•1ï¼šæŸ¥æ‰¾æ‰€æœ‰http/httpsé“¾æ¥
            pdf_urls = re.findall(r'https?://[^\s"\']+\.pdf', page_content)
            if pdf_urls:
                logger.info(f"ä»HTMLå†…å®¹ä¸­æ‰¾åˆ° {len(pdf_urls)} ä¸ªPDFé“¾æ¥")
                return pdf_urls[0]
            
            # æ–¹æ³•2ï¼šæŸ¥æ‰¾Issuuç‰¹æœ‰çš„èµ„æºé“¾æ¥
            # Issuué€šå¸¸ä½¿ç”¨documentIdæ¥æ ‡è¯†æ–‡æ¡£
            doc_id_match = re.search(r'"documentId":"([^"]+)"', page_content)
            if doc_id_match:
                doc_id = doc_id_match.group(1)
                logger.info(f"æ‰¾åˆ°æ–‡æ¡£ID: {doc_id}")
                # æ„é€ å¯èƒ½çš„ä¸‹è½½URL
                return f"https://issuu.com/api/v0/document/{doc_id}"
                
            # æ–¹æ³•3ï¼šæŸ¥æ‰¾dataå±æ€§ä¸­çš„é“¾æ¥
            data_urls = re.findall(r'data-[a-z-]+url=["\']([^"\']+)["\']', page_content, re.IGNORECASE)
            for url in data_urls:
                if 'pdf' in url.lower() or 'download' in url.lower():
                    logger.info(f"ä»dataå±æ€§æ‰¾åˆ°é“¾æ¥: {url}")
                    return url
            
        except Exception as e:
            logger.debug(f"ä»DOMæå–å¤±è´¥: {e}")
        
        logger.debug("DOMä¸­æœªæ‰¾åˆ°PDFé“¾æ¥")
        return None

    def _get_pdf_url(self, page: Page) -> Optional[str]:
        """
        ä»é¡µé¢ä¸­æå–PDFé“¾æ¥ï¼ˆå®Œæ•´ç‰ˆæœ¬ï¼‰

        Args:
            page: Playwrighté¡µé¢å¯¹è±¡

        Returns:
            PDFä¸‹è½½é“¾æ¥ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å›None
        """
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦åœ¨pageå¯¹è±¡ä¸­å­˜å‚¨äº†PDF URL
        if hasattr(page, 'pdf_url') and page.pdf_url:
            return page.pdf_url
        
        logger.debug("å¼€å§‹æŸ¥æ‰¾PDFé“¾æ¥...")
        
        # æ–¹æ³•1ï¼šæŸ¥æ‰¾PDFä¸‹è½½æŒ‰é’®æˆ–é“¾æ¥
        try:
            # å°è¯•æŸ¥æ‰¾ç›´æ¥PDFé“¾æ¥
            pdf_link = page.locator('a[href*=".pdf"]').first
            if pdf_link.is_visible(timeout=5000):
                href = pdf_link.get_attribute('href')
                if href:
                    logger.info(f"æ‰¾åˆ°PDFé“¾æ¥ (æ–¹æ³•1): {href}")
                    return href
        except Exception as e:
            logger.debug(f"æ–¹æ³•1æœªæ‰¾åˆ°PDFé“¾æ¥: {e}")

        # æ–¹æ³•2ï¼šæŸ¥æ‰¾Issuuå†…åµŒPDF viewer
        try:
            # æŸ¥æ‰¾iframeæˆ–embedå…ƒç´ 
            iframe = page.locator('iframe[src*="pdf"], embed[type*="pdf"]').first
            if iframe.is_visible(timeout=5000):
                src = iframe.get_attribute('src')
                if src:
                    logger.info(f"æ‰¾åˆ°PDFåµŒå…¥æº (æ–¹æ³•2): {src}")
                    return src
        except Exception as e:
            logger.debug(f"æ–¹æ³•2æœªæ‰¾åˆ°PDFåµŒå…¥: {e}")

        # æ–¹æ³•3ï¼šæŸ¥æ‰¾downloadæŒ‰é’®
        try:
            download_btn = page.locator('button:has-text("Download"), a:has-text("Download PDF"), button[title*="download"], button[aria-label*="download"]').first
            if download_btn.is_visible(timeout=5000):
                logger.info("æ‰¾åˆ°ä¸‹è½½æŒ‰é’®ï¼Œå°è¯•ç‚¹å‡»...")
                # ç‚¹å‡»ä¸‹è½½æŒ‰é’®
                download_btn.click()
                self._random_delay()
                # è·å–å½“å‰URLæˆ–æŸ¥æ‰¾PDFé“¾æ¥
                logger.info(f"å½“å‰é¡µé¢URL: {page.url}")
                # å°è¯•ä»é¡µé¢å˜åŒ–ä¸­è·å–ä¸‹è½½é“¾æ¥
                return page.url
        except Exception as e:
            logger.debug(f"æ–¹æ³•3ç‚¹å‡»ä¸‹è½½æŒ‰é’®å¤±è´¥: {e}")

        # æ–¹æ³•4ï¼šæŸ¥æ‰¾æ–‡æ¡£æŸ¥çœ‹å™¨ä¸­çš„PDFæº
        try:
            # æŸ¥æ‰¾classåŒ…å«docViewerçš„å…ƒç´ 
            viewer = page.locator('.document-viewer, .issuu-viewer, [class*="viewer"]').first
            if viewer.is_visible(timeout=5000):
                logger.debug("æ‰¾åˆ°æ–‡æ¡£æŸ¥çœ‹å™¨")
                # è·å–viewerå†…çš„iframe
                pdf_frame = viewer.locator('iframe').first
                if pdf_frame.is_visible(timeout=3000):
                    src = pdf_frame.get_attribute('src')
                    if src:
                        logger.info(f"æ‰¾åˆ°viewerå†…çš„PDFæº (æ–¹æ³•4): {src}")
                        return src
        except Exception as e:
            logger.debug(f"æ–¹æ³•4æœªæ‰¾åˆ°viewer: {e}")

        # æ–¹æ³•5ï¼šæ‰§è¡ŒJavaScriptæŸ¥æ‰¾PDFèµ„æº
        try:
            logger.debug("å°è¯•é€šè¿‡JavaScriptæŸ¥æ‰¾PDFèµ„æº...")
            js_code = """
            () => {
                // æŸ¥æ‰¾æ‰€æœ‰åŒ…å«pdfçš„URL
                const links = Array.from(document.querySelectorAll('a[href]'));
                const pdfLinks = links.filter(a => a.href.includes('.pdf'));
                if (pdfLinks.length > 0) return pdfLinks[0].href;
                
                // æŸ¥æ‰¾æ‰€æœ‰iframeå’Œembed
                const frames = Array.from(document.querySelectorAll('iframe, embed'));
                for (const frame of frames) {
                    if (frame.src && (frame.src.includes('pdf') || frame.src.includes('document'))) {
                        return frame.src;
                    }
                }
                
                // æŸ¥æ‰¾windowå¯¹è±¡ä¸­çš„é…ç½®
                if (window.__INITIAL_STATE__ && window.__INITIAL_STATE__.document) {
                    return window.__INITIAL_STATE__.document.url;
                }
                
                return null;
            }
            """
            pdf_url = page.evaluate(js_code)
            if pdf_url:
                logger.info(f"JavaScriptæ‰¾åˆ°PDFé“¾æ¥ (æ–¹æ³•5): {pdf_url}")
                return pdf_url
        except Exception as e:
            logger.debug(f"æ–¹æ³•5 JavaScriptæŸ¥æ‰¾å¤±è´¥: {e}")

        # æ–¹æ³•6ï¼šä»æ‰€æœ‰é“¾æ¥ä¸­æŸ¥æ‰¾
        try:
            all_links = page.locator('a[href]').all()
            logger.debug(f"æ‰¾åˆ° {len(all_links)} ä¸ªé“¾æ¥")
            for link in all_links:
                href = link.get_attribute('href')
                if href and ('pdf' in href.lower() or 'download' in href.lower()):
                    logger.info(f"æ‰¾åˆ°å¯èƒ½çš„ä¸‹è½½é“¾æ¥ (æ–¹æ³•6): {href}")
                    return href
        except Exception as e:
            logger.debug(f"æ–¹æ³•6æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥å¤±è´¥: {e}")

        logger.warning("æ‰€æœ‰æ–¹æ³•éƒ½æœªèƒ½æ‰¾åˆ°PDFä¸‹è½½é“¾æ¥")
        return None

    def _download_file(self, url: str, save_path: str) -> bool:
        """
        ä¸‹è½½æ–‡ä»¶

        Args:
            url: æ–‡ä»¶URL
            save_path: ä¿å­˜è·¯å¾„

        Returns:
            æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        download_start = time.time()
        
        try:
            headers = {
                'User-Agent': self._get_random_user_agent(),
                'Accept': 'application/pdf,*/*',
                'Accept-Language': 'en-US,en;q=0.9',
                'Referer': 'https://issuu.com/',
            }
            
            logger.info(f"å¼€å§‹ä¸‹è½½: {url}")
            
            # å¦‚æœä½¿ç”¨ä»£ç†
            proxies = None
            if self.proxy_url:
                proxies = {
                    'http': self.proxy_url,
                    'https': self.proxy_url,
                }
                logger.info(f"ä½¿ç”¨ä»£ç†: {self.proxy_url}")
            
            response = requests.get(
                url, 
                headers=headers, 
                timeout=self.timeout / 1000,  # è½¬æ¢ä¸ºç§’
                stream=True,
                proxies=proxies
            )
            response.raise_for_status()

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(save_path), exist_ok=True)

            # ä¿å­˜æ–‡ä»¶
            total_size = int(response.headers.get('content-length', 0))
            downloaded = 0
            
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        if total_size > 0:
                            progress = (downloaded / total_size) * 100
                            if progress % 10 < 1:  # æ¯10%è¾“å‡ºä¸€æ¬¡
                                logger.debug(f"ä¸‹è½½è¿›åº¦: {progress:.1f}%")

            elapsed = time.time() - download_start
            file_size = os.path.getsize(save_path)
            
            logger.info(f"âœ… æ–‡ä»¶å·²ä¿å­˜: {save_path}")
            logger.info(f"   å¤§å°: {file_size / 1024:.2f} KB")
            logger.info(f"   è€—æ—¶: {elapsed:.2f}ç§’")
            
            return True

        except requests.exceptions.ProxyError as e:
            logger.error(f"âŒ ä»£ç†é”™è¯¯: {e}")
            logger.error(f"   ä»£ç†URL: {self.proxy_url}")
            return False
        except requests.exceptions.Timeout as e:
            logger.error(f"âŒ ä¸‹è½½è¶…æ—¶: {e}")
            return False
        except requests.exceptions.HTTPError as e:
            logger.error(f"âŒ HTTPé”™è¯¯: {e}")
            logger.error(f"   çŠ¶æ€ç : {e.response.status_code if hasattr(e, 'response') else 'N/A'}")
            return False
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
            return False

    def _download_direct_url(self, url: str, save_path: str) -> bool:
        """
        ç›´æ¥ä¸‹è½½URLï¼ˆä¸ä½¿ç”¨æµè§ˆå™¨ï¼Œç”¨äºå·²çŸ¥PDFé“¾æ¥ï¼‰

        Args:
            url: PDFä¸‹è½½é“¾æ¥
            save_path: ä¿å­˜è·¯å¾„

        Returns:
            æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        logger.info("å°è¯•ç›´æ¥ä¸‹è½½URL...")
        return self._download_file(url, save_path)

    def download_pdf(self, url: str, save_path: str) -> bool:
        """
        ä¸‹è½½PDFæ–‡ä»¶

        Args:
            url: Issuuæ–‡æ¡£URL
            save_path: æœ¬åœ°ä¿å­˜è·¯å¾„

        Returns:
            æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹ä¸‹è½½PDF")
        logger.info("=" * 60)
        logger.info(f"URL: {url}")
        logger.info(f"ä¿å­˜è·¯å¾„: {save_path}")
        logger.info("=" * 60)

        total_start = time.time()

        for attempt in range(1, self.max_retries + 1):
            attempt_start = time.time()
            logger.info(f"\nğŸ”„ å°è¯• {attempt}/{self.max_retries}")
            logger.info("-" * 60)

            try:
                self._random_delay()

                with sync_playwright() as p:
                    # å¯åŠ¨æµè§ˆå™¨
                    browser = p.chromium.launch(
                        headless=self.headless,
                        args=BROWSER_ARGS
                    )
                    
                    # åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡
                    context_options = {
                        'user_agent': self._get_random_user_agent(),
                        'viewport': {'width': VIEWPORT_WIDTH, 'height': VIEWPORT_HEIGHT},
                        'locale': LOCALE,
                        'timezone_id': TIMEZONE_ID,
                    }
                    
                    # æ·»åŠ ä»£ç†é…ç½®
                    if self.proxy_url:
                        context_options['proxy'] = {'server': self.proxy_url}
                        logger.info(f"âœ“ æµè§ˆå™¨å·²é…ç½®ä»£ç†: {self.proxy_url}")
                    
                    context = browser.new_context(**context_options)
                    page = context.new_page()

                    # é…ç½®é¡µé¢
                    self._setup_page(page)

                    # ä½¿ç”¨å¤šçº§ç­‰å¾…ç­–ç•¥æ‰“å¼€é¡µé¢
                    page_loaded = self._open_page_with_strategy(page, url)

                    if not page_loaded:
                        logger.error(f"âŒ é¡µé¢åŠ è½½å¤±è´¥ (å°è¯• {attempt}/{self.max_retries})")
                        browser.close()
                        continue

                    # æŸ¥æ‰¾PDFé“¾æ¥
                    logger.info("æ­£åœ¨æŸ¥æ‰¾PDFä¸‹è½½é“¾æ¥...")
                    pdf_url = self._get_pdf_url(page)

                    if pdf_url:
                        # å¦‚æœæ˜¯ç›¸å¯¹URLï¼Œè½¬æ¢ä¸ºç»å¯¹URL
                        if pdf_url.startswith('/'):
                            from urllib.parse import urljoin
                            pdf_url = urljoin(url, pdf_url)

                        logger.info(f"âœ… æ‰¾åˆ°PDFé“¾æ¥: {pdf_url}")
                        logger.info(f"å¼€å§‹ä¸‹è½½PDFæ–‡ä»¶...")
                        
                        if self._download_file(pdf_url, save_path):
                            elapsed = time.time() - total_start
                            logger.info("=" * 60)
                            logger.info(f"âœ… PDFä¸‹è½½æˆåŠŸ! æ€»è€—æ—¶: {elapsed:.2f}ç§’")
                            logger.info("=" * 60)
                            browser.close()
                            return True
                    else:
                        logger.warning(f"âš ï¸  æœªæ‰¾åˆ°PDFé“¾æ¥ (å°è¯• {attempt}/{self.max_retries})")

                    browser.close()

            except PlaywrightTimeoutError as e:
                logger.warning(f"â±ï¸  é¡µé¢åŠ è½½è¶…æ—¶ (å°è¯• {attempt}/{self.max_retries}): {e}")
            except Exception as e:
                logger.error(f"âŒ ä¸‹è½½å¤±è´¥ (å°è¯• {attempt}/{self.max_retries}): {e}")
                import traceback
                logger.debug(traceback.format_exc())

            # é‡è¯•å‰ç­‰å¾…
            if attempt < self.max_retries:
                wait_time = random.uniform(5, 10)
                logger.info(f"ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                time.sleep(wait_time)

        total_elapsed = time.time() - total_start
        logger.error("=" * 60)
        logger.error(f"âŒ PDFä¸‹è½½å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {self.max_retries}")
        logger.error(f"   æ€»è€—æ—¶: {total_elapsed:.2f}ç§’")
        logger.error("=" * 60)
        return False
